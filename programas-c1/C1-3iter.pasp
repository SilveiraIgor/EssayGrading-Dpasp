#python
import torch
from datasets import load_dataset
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class MNISTNet(nn.Module):
  def __init__(self, name, numero_labels, exibir):
    super(MNISTNet, self).__init__()
    self.exibir = exibir
    self.rede = AutoModelForSequenceClassification.from_pretrained(
                name,
                cache_dir="/tmp/aes_enem2",
                num_labels=numero_labels)
    TOKENIZER_NAME = f"neuralmind/bert-base-portuguese-cased"
    self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
  def forward(self, x):
    x, y, z = torch.unbind(x, dim=1) #ids, token_type_ids, attention_mask
    x = x.long()
    y = y.long()
    z = z.long()
    #entrada = self.tokenizer(x, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
    #output1 = self.rede(input_ids=entrada[0], token_type_ids=entrada[1],  attention_mask=entrada[2])
    #print(f"Formato que chegou no {self.exibir}: ", x.shape, x.dtype)
    #decodado = self.tokenizer.batch_decode(x, skip_special_tokens=True)
    #for d in decodado:
      #print(">>O texto que chegou foi: ", d[:20])
    #print("Devices: ", x.device, y.device, z.device, self.rede.device)
    output1 = self.rede(input_ids=x, token_type_ids=y, attention_mask=z)
    resposta = F.softmax(output1.logits, dim=1)
    #print("Logits do BERT: ", resposta)
    #print(f"Formato da resposta do {self.exibir}: ", resposta.shape, resposta.dtype)
    #print(resposta.device)
    return resposta

def digit_net_1(): return MNISTNet("igorcs/Syntax-A", numero_labels=5, exibir="Syntaxe") #.to("cuda")
def digit_net_2(): return MNISTNet("igorcs/Mistakes-A", numero_labels=4, exibir="Mistakes") #.to("cuda")

# Retrieve the MNIST data.
def mnist_data(): 
  ds = load_dataset("kamel-usp/aes_enem_dataset", "JBCS2025", cache_dir="/tmp/aes_enem")
  train = ds['train']
  test  = ds['test']
  return train, test

def normalize(train, test):
  TOKENIZER_NAME = f"neuralmind/bert-base-portuguese-cased"
  tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
  lista_train, lista_test = [], []
  respostas_train, respostas_test = [], []
  for linha in train:
    respostas_train.append(linha['grades'][0]//40)
  for linha in test:
    respostas_test.append(linha['grades'][0]//40)
  lista_train = tokenizer(train[:]['essay_text'], return_tensors="pt", truncation=True, padding='max_length', max_length=512)
  lista_test = tokenizer(test[:]['essay_text'], return_tensors="pt", truncation=True, padding='max_length', max_length=512)
  #print(lista_train['input_ids'])
  #print(lista_train['input_ids'].dtype, lista_train['token_type_ids'].dtype, lista_train['attention_mask'].dtype)
  lista_train = torch.stack([lista_train['input_ids'], lista_train['token_type_ids'], lista_train['attention_mask']], dim=1)
  lista_test = torch.stack([lista_test['input_ids'], lista_test['token_type_ids'], lista_test['attention_mask']], dim=1)
  #print("Sobre o lista_train: ", lista_train.shape, lista_train.ndim, lista_train.dtype)
  respostas_train = torch.tensor(respostas_train[:], dtype=torch.int)
  respostas_test = torch.tensor(respostas_test[:], dtype=torch.int)
  #print("As respostas deveriam ser: ", respostas_train)
  #print("Sobre o respostas: ", respostas_train.shape, respostas_train.ndim, respostas_train.dtype)
  return lista_train, respostas_train, lista_test, respostas_test

train, test = mnist_data()
train_X, train_Y, test_X, test_Y = normalize(train, test)
#print("Shape do train_X: ", train_X.shape)
#print("Shape do train_Y: ", train_Y.shape)
#print("Shape do test_X: ", test_X.shape)
#print("Shape do test_Y: ", test_Y.shape)
#return train_X, train_Y, test_X, test_Y

# MNIST images for the train set.
def mnist_images_train(i):
  t_X, t_Y, te_X, te_Y = train_X, train_Y, test_X, test_Y
  #print("No mnist_images_train, os tensores tem tamanho: ", t_X.shape)
  return t_X #.to("cuda")
# MNIST images for the test set.
def mnist_images_test(i):
  t_X, t_Y, te_X, te_Y = train_X, train_Y, test_X, test_Y
  #print("No mnist_images_test, os tensores tem tamanho: ", te_X.shape)
  return te_X #.to("cuda")

# Observed atoms for training.
def mnist_labels_train():
  # We join the two halves (top and bottom) of MNIST and join them together to get
  # two digits side by side. The labels are atoms encoding the sum of the two digits.
  labels = train_Y
  resposta = [[f"grade({y})"] for y in labels]
  #resposta = [f"grade({y})" for y in labels]
  #print("No mnist_labels_train: ", resposta)
  return resposta
#end.

% Data of the first digit.
input(0) ~ test(@mnist_images_test(0)), train(@mnist_images_train(0)).
input(1) ~ test(@mnist_images_test(1)), train(@mnist_images_train(1)).

% Neural angradeted disjunction over each digit from 0 to 9; use Adam as optimizer
% and a learning rate of 0.001.
?::digit2(X, {0..3}) as @digit_net_2 with optim = "Adam", lr = 0.00001 :- input(X).
?::digit1(X, {0..4}) as @digit_net_1 with optim = "Adam", lr = 0.00001 :- input(X).
% The sum
grade(0) :- digit1(0,0).
grade(1) :- digit1(0,1), digit2(0,0).
%dois casos de grade 2
grade(2) :- digit1(0,1), digit2(0,B), B>=1.
grade(2) :- digit1(0,A), digit2(0,0), A>=2.
%dois casos de grade 3
grade(3) :- digit1(0,2), digit2(0,B), B>=1.
grade(3) :- digit1(0,A), digit2(0,1), A>=2.
%dois casos de grade 4
grade(4) :- digit1(0,3), digit2(0,B), B>=2.
grade(4) :- digit1(0,A), digit2(0,2), A>=3.
%caso unico pra grade 5
grade(5) :- digit1(0,4), digit2(0,3).

% Learn the parameters of the program from the "sum(X)" atoms.
#learn @mnist_labels_train, lr = 1., niters = 3, alg = "lagrange", batch = 4.
#semantics maxent.
% Ask for the probability of all groundings of sum(X).
#query grade(0). 
#query grade(1).
#query grade(2).
#query grade(3).
#query grade(4).
#query grade(5).
