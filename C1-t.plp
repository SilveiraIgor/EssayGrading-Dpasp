#python
import torch
from datasets import load_dataset
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class MNISTNet(nn.Module):
  def __init__(self, name, numero_labels):
    super(MNISTNet, self).__init__()
    self.rede = AutoModelForSequenceClassification.from_pretrained(
                name,
                cache_dir="/tmp/aes_enem2",
                num_labels=numero_labels)
    TOKENIZER_NAME = f"neuralmind/bert-base-portuguese-cased"
    self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
  def forward(self, x):
    x = x.long()
    #entrada = self.tokenizer(x, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
    #output1 = self.rede(input_ids=entrada[0], token_type_ids=entrada[1],  attention_mask=entrada[2])
    print("Vou printar o formato do vetor que chegou no forward bert: ", x.shape, x.dtype)
    decodado = self.tokenizer.batch_decode(x, skip_special_tokens=True)
    for d in decodado:
      print(">>O texto que chegou foi: ", d[:20])
    output1 = self.rede(input_ids=x)
    resposta = F.softmax(output1.logits, dim=1)
    #print("Logits do BERT: ", resposta)
    return resposta

def digit_net_1(): return MNISTNet("igorcs/Syntax-A", numero_labels=5)
def digit_net_2(): return MNISTNet("igorcs/Mistakes-A", numero_labels=4)

# Retrieve the MNIST data.
def mnist_data(): 
  ds = load_dataset("kamel-usp/aes_enem_dataset", "JBCS2025", cache_dir="/tmp/aes_enem")
  train = ds['train']
  test  = ds['test']
  return train, test

def normalize(train, test):
  TOKENIZER_NAME = f"neuralmind/bert-base-portuguese-cased"
  tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
  lista_train = []
  respostas_train = []
  for linha in train:
    respostas_train.append(linha['grades'][0]//40)
  lista_train = tokenizer(train[:2]['essay_text'], return_tensors="pt", truncation=True, padding='max_length', max_length=512)#train['essay_text']
  #print(lista_train['input_ids'])
  lista_train = lista_train['input_ids']
  print("Sobre o lista_train: ", lista_train.shape, lista_train.ndim, lista_train.dtype)
  respostas_train = torch.tensor(respostas_train[:2], dtype=torch.int)
  print("As respostas deveriam ser: ", respostas_train)
  print("Sobre o respostas: ", respostas_train.shape, respostas_train.ndim, respostas_train.dtype)
  return lista_train, respostas_train, lista_train, respostas_train

def gerar_dataset():
  train, test = mnist_data()
  train_X, train_Y, test_X, test_Y = normalize(train, test)
  return train_X, train_Y, test_X, test_Y

# MNIST images for the train set.
def mnist_images_train(i):
  print("Vou gerar um dataset no Train")
  t_X, t_Y, _, _ = gerar_dataset()
  return t_X
# MNIST images for the test set.
def mnist_images_test(i):
  print(">>Vou gerar um dataset porque estou no Teste")
  return mnist_images_train(i)

# Observed atoms for training.
def mnist_labels_train():
  # We join the two halves (top and bottom) of MNIST and join them together to get
  # two digits side by side. The labels are atoms encoding the sum of the two digits.
  
  _, labels, _, _ = gerar_dataset()
  resposta = [[f"grade({y})"] for y in labels]
  #resposta = [f"grade({y})" for y in labels]
  print("No mnist_labels_train: ", resposta)
  return resposta
#end.

% Data of the first digit.
input(0) ~ test(@mnist_images_test(0)), train(@mnist_images_train(0)).
input(1) ~ test(@mnist_images_test(1)), train(@mnist_images_train(1)).

% Neural angradeted disjunction over each digit from 0 to 9; use Adam as optimizer
% and a learning rate of 0.001.
?::syntax(X, {0,1,2,3,4}) as @digit_net_1 with optim = "Adam", lr = 0.001 :- input(X).
?::mistakes(X, {0,1,2,3}) as @digit_net_2 with optim = "Adam", lr = 0.001 :- input(X).
% The sum
grade(0) :- syntax(0,0).
grade(1) :- syntax(0,1), mistakes(1,0).
%dois casos de grade 2
grade(2) :- syntax(0,1), mistakes(1,B), B>=1.
grade(2) :- syntax(0,A), mistakes(1,0), A>=2.
%dois casos de grade 3
grade(3) :- syntax(0,2), mistakes(1,B), B>=1.
grade(3) :- syntax(0,A), mistakes(1,1), A>=2.
%dois casos de grade 4
grade(4) :- syntax(0,3), mistakes(1,B), B>=2.
grade(4) :- syntax(0,A), mistakes(1,2), A>=3.
%caso unico pra grade 5
grade(5) :- syntax(0,4), mistakes(1,3).

% Learn the parameters of the program from the "sum(X)" atoms.
#learn @mnist_labels_train, lr = 0.1, niters = 1, alg = "lagrange", batch = 1.
#semantics maxent.
% Ask for the probability of all groundings of sum(X).
%#query grade(X).
#query grade(0). 
#query grade(1).
#query grade(2).
#query grade(3).
#query grade(4).
#query grade(5).
